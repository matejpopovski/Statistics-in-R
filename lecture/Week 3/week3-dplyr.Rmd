---
title: "STAT 240: dplyr"
author: "Cameron Jones"
date: "Spring 2024"
output: rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE,  message=FALSE, warning=FALSE,
                      error = TRUE,
                      fig.height = 4)
library(tidyverse)
library(lubridate)
library(viridisLite)
library(magrittr)
```


# Overview

## Learning Outcomes

* These lectures will teach you how to:
  - Use appropriate vocabulary to describe the form of data
  - Execute basic manipulations on dataframes, such as rearranging and creating new columns
  - Execute more complex manipulations of dataframes, such as summarizing and pivoting

## Preliminaries

- For you to be able to run the code in this file,
you need to put files in proper locations and might need to set the working directory.
- Let `COURSE` represent your course directory.

1. If it does not yet exist, create the folder `COURSE/lecture`.
2. Create the sub-folder `COURSE/lecture/unit3-dplyr/`
3. Download the file `week3-dplyr.Rmd` into the `unit3-dplyr` sub-folder.
4. Download the files `grocery-prices.csv`, `grocery-list.csv`, `lake-mendota-raw-2022.csv`, and `madison-weather-1869-2022.csv` into the `COURSE/data/` folder.

* You will need the `viridisLite` and `magrittr` packages for this file; run `install.packages("viridisLite")` and `install.packages("magrittr")` in your console. You will also need `lubridate` and `tidyverse` which you should have installed for a previous lecture.

# An Introduction to dplyr

* In the `ggplot2` unit, data was given to you in a nice, convenient form, with all the information you need present and intuitively named.

* Data from external sources almost always requires *cleaning* or *wrangling*; that is, it comes in a different form or with less information than you need, and you have to do some work to get it into a convenient form.

* [dplyr](https://dplyr.tidyverse.org/) contains a wide range of functions to manipulate and transform dataframes.
    + The [dplyr cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf) is a useful summary of everything `dplyr` can do - we won't get through it all in this course.
    
## Principles of dplyr

* Like `ggplot2`, `dplyr` is so powerful because **the intelligent combination of a small set of simple building blocks supports a wide range of actions**.

* We are going to set the framework for this elegant combination system by looking at the pipe operator (`%>%`), then preview each building block on its own, and finally start combining these commands for more advanced results.

## The Pipe

* The pipe operator is written out as `%>%`.
    + I encourage you to get in the habit of instead using the keyboard shortcut, `Ctrl` + `Shift` + `M` on Windows or `Command` + `Shift` + `M` on Mac; you'll be writing hundreds of pipes this semester!
    
> The pipe operator takes an object on the left and passes it into the function on the right as its FIRST argument.

* Conceptually, this means `x %>% f()` is the same as `f(x)`, and crucially, `x %>% f(y, z)` is equivalent to `f(x, y, z)`, NOT `f(y, z, x)`.

```{r}
# These are doing exactly the same thing!
subtract(5, 2) # 5 - 2 = 3
5 %>% subtract(2) # This is 5 - 2 = 3, not 2 - 5 = -3
```

* Why would anyone create or use this operator? It seems a little silly... until you use it to chain functions together.

* If you write `f(x) %>% g()`, this is the same as `g(f(x))`, where the result of `f(x)`  (could be another number, a vector, or eventually a dataframe) is passed in as the first argument to `g`. 

* Similarly, `f(x) %>% g(y)` is equivalent to `g(f(x), y)`.

* Extending one level further, `f(x) %>% g(y) %>% h(z)` is equivalent to `h(g(f(x), y), z)`; and you start to see the value of the pipe.

* Applying this to the first example, say we wanted to use functions to start with 5, then subtract 2, then multiply the result by 4, then square the result, then divide the result by 12.

* This leads to a very natural conceptualization of the pipe; **it works like the word "then"**.

```{r}
# Without the pipe
divide_by(raise_to_power(multiply_by(subtract(5, 2), 4), 2), 12)

# With the pipe
5 %>% 
  subtract(2) %>% 
  multiply_by(4) %>% 
  raise_to_power(2) %>% 
  divide_by(12)
```

*Note: You would never actually use these functions in a real context, you would just use the simple mathematical operators `-`, `^`, et cetera. This is merely for example since we're covering the pipe before dplyr commands.*

* In this sense, the pipe is not technically necessary. Anything you can do with it, you can do without it; but without the pipe, your code will be very difficult to read, even harder to write, and harder still to troubleshoot if it doesn't work.

---

##### EXERCISE: Basic Use of The Pipe

* This exercise will give you experience writing with the pipe and tracing through how it works.

* A reminder the keyboard shortcut for `%>%` is `Ctrl`+`Shift`+`M` on Windows or `Command`+`Shift`+`M` on Mac.

* Work out exactly what the following code is doing, and recreate it with the pipe.

* Recall that the `sqrt()` function takes only a single argument, and returns its (positive) square root.

```{r}
# What is this doing?
sqrt(add(divide_by(30, 10), 6))

# Recreate it with the pipe, with each function on its own line

```

> Technical takeaway: Using the pipe makes code easier to write and read.

> Philosophical takeaway: The pipe connects dplyr commands in the same conceptual way that + connects ggplot2 commands.

---

* Because each `dplyr` command takes a dataframe in as its first argument, and returns a dataframe, multiple commands can be run in a chain, each working from the output of the previous command.

# The Basic dplyr Commands

* Each `dplyr` command follows this basic set of three rules:

> The first argument is the dataframe to manipulate.

> The rest of the arguments detail how you want to manipulate that dataframe.

> The result of the command is the edited dataframe; which is often passed to the next function as its first argument with the pipe.

* Because the result of each `dplyr` command can act as the first argument of another, they can be naturally chained together with the pipe.

* We'll get to examples of this eventually, but we need to go over each individual building block command on its own.

* There are dozens and dozens of `dplyr` commands that we will see over the course of the semester; this lecture will serve as an introduction to the most important subset.

## Read in Data

```{r}
grocery_prices = read_csv("../../data/grocery-prices.csv")
grocery_list = read_csv("../../data/grocery-list.csv")

grocery_prices
grocery_list
```

## The Join Commands

* **We will revisit joining in more detail in week 5,** but we provide a description here for the curious.

- **dplyr** contains several `*_join` commands to merge together COLUMNS from two different data sets which have some ROWS in common.

- These commands require three things:
    - The two dataframes to merge together (as the first two arguments)
    - The names of the column(s) which the dataframes have in common (as the `by` argument)
        - dplyr can sometimes guess what you mean if you don't specify `by`, but it is best practice to specify it.
        
- Consider the 10 items in `grocery_list` above. We want to add columns for the `price` and `type` of each item, which are right now stored in `grocery_prices`.

- The values in `item` in `grocery_list` match up with the values in `item` in `grocery_prices`, so we will add the argument `by = "item"` to the join we end up using.
    + This is one of very few cases where we refer to a column with quotes, we almost always do not.

- The four different join commands do subtly different things:
    - `left_join(x, y)` keeps all the rows in `x`  regardless of matching, and appends columns where possible from `y`.
    - `right_join(x, y)` keeps all the rows in `y` regardless of matching, and appends columns where possible from `x`.
    - `inner_join(x, y)` only keeps those rows which appear in BOTH `x` and `y`.
    - `full_join(x, y)` keeps ANY row which appears in either dataframe.

- Some of these commands may result in missing values; In this case, the default is to plug in `NA`, but you can optionally specify this as well.

```{r}
produce = grocery_list %>% 
  left_join(grocery_prices, by = "item") 

produce
```

- Note that what we just did was the same as `left_join(grocery_list, grocery_prices)`.

## The Column Editors

> The following four commands, `mutate()`, `select()`, `relocate()` and `rename()`, act upon the COLUMNS of dataframes, leaving the number and order of rows alone.

### mutate()

> `mutate()` adds a new column or columns to the dataframe.

- The secondary arguments to `mutate` (recall the first is the initial dataframe) are as many or as few expressions as you want, each of the form `newColumnName = expression`, separated by commas if there are multiple.

- `expression` is often (but not always) based on other columns in the dataframe.

```{r}
produce = produce %>% 
    mutate(cost = price * quantity,
           sales_tax = 0.05 * cost)

produce
```

### select()

> `select()` reduces the number of columns in the dataframe, by giving it a list of columns to keep or to remove.

- In its most basic form, `select()` takes a list of columns (not in quotes) and will keep them in the dataframe, throwing out the rest.

```{r}
# Will KEEP cost and sales_tax only, removing the rest
produce %>%
  select(cost, sales_tax)
```

- You can also tell `select()` you instead wish to REMOVE the specified columns using the `-` sign before each name.

```{r}
produce %>% 
  select(-item, -type)
```

### relocate()

> `relocate()` moves columns around in a dataframe, not removing or adding any.

- By default, `relocate()` takes a list of specified columns and brings them to the immediate left of the dataframe.

```{r}
produce %>% 
  relocate(price, quantity, cost, sales_tax)
```

- You can also choose to move columns to other specific places using the `.before` or `.after` arguments.

```{r}
produce %>% 
  relocate(item, type, .after = price)
```

### rename()

> rename() changes the names of columns, using the syntax `new_name = old_name`.

* The secondary arguments to `rename()` are as many or as few pairs of `new_name = old_name` as you want, separated by commas if multiple.

```{r}
produce %>% 
  rename(pricePerUnit = price,
         totalCost = cost)
```


## The Row Editors

> The following two commands, `arrange()` and `filter()`, act upon the ROWS of dataframes, leaving the number and order of columns alone.

### arrange()

> `arrange()` reorders the rows by the value of some variable in the dataframe.

- `arrange()` in its most basic form accepts a single variable which has values that can be compared (numerically, dates/times, alphabetically), and orders rows from smallest to largest based on their value that variable.

```{r}
produce %>% 
  arrange(cost)
```

- If you want to go from largest to smallest, use the `desc()` helper function around the variable.

```{r}
# String variables can be compared alphabetically!

# Think of words starting with 'a' as the number 1, and words starting with 'z' as the number 26. 

produce %>%
  arrange(desc(item))
```

* You can also provide further variables to break ties in the first one, if desired. If not, the order in the original dataframe will break the tie.

### filter()

> `filter()` keeps rows which satisfy a specified logical condition.

- Recall that "logical conditions" are achieved with the logical operators, such as `<`, `>`, and `==`, among others.

- They must evaluate to `TRUE` or `FALSE` on each row.

```{r}
# Keeps rows where the value of type in that row is equal to the literal word "fruit"
produce %>% 
  filter(type == "fruit")
```

- More complicated logical conditions can be achieved with the use of the symbols `&` (read as 'and') as well as `|` (read as 'or', top right of the keyboard)

- For example, to keep just the rows where `cost` is between 2 and 5:

```{r}
# We connect these two logical conditions with "&" because we want to keep rows only where BOTH are satisfied.

# For example, a row where cost is 1.5 satisfies the second condition, but not the first; we want to remove that.

produce %>% 
  filter(cost > 2 & cost < 5)
```

- To keep the rows where cost is OUTSIDE the 2 to 5 dollar range:

```{r}
# We connect these logical conditions with OR because we want to keep rows where EITHER is true. 

# For example, a row where cost is 1.5 satisfies the first condition, but not the second. We want to keep it.
produce %>% 
  filter(cost < 2 | cost > 5)
```

---

##### EXERCISE: dplyr in Translation

* Real conversations with non-coders will often not use the name of the verb; you will have to take their conversational speech and "translate" it into commands.

* **Choose the correct dplyr function** for each of the following "plain English" requests. If you can, try to imagine what arguments the command would request.

* *"Show me just the fruits."*

* *"Can you put whether it's a fruit or vegetable at the beginning?"*

* *"I only care about the food name and the total cost, can you get rid of everything else?."*

* *"Can you put the foods in alphabetical order?"*

* *"Can you give each food a sale price that's half off the normal price?"*

---

## summarize()

* `summarize()` is entirely different from the previous column and row editors, in that it will edit the content and number of both rows AND columns. 

> `summarize()` computes "summary expressions" which each take a column or columns and reduce them to one number. 

* Just like `mutate()`, summarize takes a list of arguments in the form `newColumnName = expression`.

* However, the key difference is that `mutate()` creates a new column the same length as the existing ones. `summarize()` computes expressions which **reduce a column down to just one value**.

* It is important to understand going into `summarize()` whether you want a single number of output or a column the same length as the input.

* As a reminder, here are some examples of operators which will return vectors the SAME length as the input.

```{r}
primeNumbers = c(2, 3, 5, 7, 11)
fibonacciNumbers = c(0, 1, 1, 2, 3)

primeNumbers + fibonacciNumbers

primeNumbers^2

primeNumbers > fibonacciNumbers

```

* And here are some functions which take a vector of numbers and return a SINGLE number, no matter the length of the input.
    + I often call these *reducing functions*, or *summarizing functions*.

```{r}
sum(primeNumbers)

max(fibonacciNumbers)

mean(primeNumbers)
```

* Back to `summarize()`; the `expression` on the right side of each pair must be a *reducing function* of an existing column or columns.

```{r}
produce %>% 
  summarize(totalCost = sum(cost),
            totalTax = sum(sales_tax))
```

* Notice some important results of `summarize()`:
    - We have lost every existing column, and are left with ONLY the ones we created in `summarize()`
    - We have lost all of the specific information of the dataframe, and now only have a summary of the original
    
* One might wonder why this command is helpful, because what we just did can also be achieved with:

```{r}
sum(produce$cost)
sum(produce$sales_tax)
```

* `summarize()` in particular becomes much more useful when combined with `group_by`, which we are about to see.

# group_by + Combining Dplyr Commands

* `group_by` is unique in that it does not change ANY content of the dataframe; not the number, order, or content of rows, nor columns.

> `group_by` serves as an instruction for future commands, to evaluate expressions within each group, rather than across the whole dataframe.

* The only thing `group_by()` does is set the "grouping" property of the dataframe behind the scenes, not changing any of the content.

* Notice the `Groups: type [2]` at the top of the following output. This is because there are two different values in the column `type`.

```{r}
produce %>% 
  group_by(type)
```

* Let's say you wanted to find the average `price` of the fruits in the dataframe, and the average `price` of vegetables in the dataframe.

* A naive, but reasonable, approach:

```{r}
# Live coding
```

* You can see how this would become tedious if you had more than two groups, or you wanted to calculate more than one thing. 

* Furthermore, also notice that we are doing exactly the same operation; `summarize(averagePrice = mean(price))`; within each group. 

* `group_by(g)` tells future commands that they should evaluate expressions within each group defined by `g`.
    - So, `group_by(type)`, followed by a command with an expression, will compute that expression among all the `type == "fruit"` rows, and then separately among all the `type == "vegetable"` rows.

```{r}
produce %>% 
  group_by(type) %>%
  summarize(averagePrice = mean(price))
```

```{r}
produce %>% 
  group_by(type) %>% 
  summarize(averagePrice = mean(price),
            lowestCost = min(cost),
            highestCost = max(cost))
```

* Each *reducing function* is being evaluated within each group, not across the whole dataframe, which was the behavior of `summarize()` without defined groups.

* Notice that after `summarize()`, the grouping instruction has disappeared. It no longer makes sense to have groups defined, because we only have summary information.

## n(), a helpful reducing function

* In the `group_by() %>% summarize()` workflow, a common and helpful reducing function is `n()`.

> `n()` returns the number of rows in each defined group.

* It takes no arguments; it infers what you want to count by the presence of a previous `group_by`.

```{r}
produce %>% 
  group_by(type) %>%
  summarize(numItems = n(), averagePrice = mean(price))
  
```

### count(), a shortcut for n()

> `count(g)` is an optional shortcut for `group_by(g) %>% summarize(n = n())`.

* The flow of `group_by(g) %>% summarize(numRows = n())` is such a common and useful operation that `tidyverse` established the shortcut command `count(g)`.

* `count()` can make your code easier to read; but if you need to calculate other summaries as well, you'll need to use the full `summarize()`.

```{r}
# Produces the same counts (6 fruits, 4 vegetables) as n() above; increases readability, sacrifices some flexibility.

produce %>% 
  count(type)
```

## Extensions of group_by()

* `summarize()` is not the only function which behaves differently with a grouped dataframe; any commands which do any evaluation will obey grouping.

### Applications to mutate()

* Recall that we can give `mutate()` a single number, and it will be extended to the length of the existing columns.

```{r}
produce %>% 
  mutate(newColumn = 5) %>% 
  relocate(newColumn)
```

* If we provide `mutate()` with a reducing function like `sum(price)` or `min(price)`; the output of that is a single number, so it is very similar to the above operation.

```{r}
produce %>% 
  mutate(totalCost = sum(cost)) %>% 
  relocate(totalCost)
```

* If we add a `group_by`, we can assign each row a single number; but that single number will be different depending on which group the row is in.

```{r}
produce %>% 
  group_by(type) %>% 
  mutate(totalGroupCost = sum(cost)) %>% 
  relocate(totalGroupCost)
```

* This can be useful in determining what percentage of a group a certain row accounts for; you can append the *within-group* sum to each row with a grouping function and `sum` (the denominator of the fraction), and then calculate the *percentage* by dividing the individual row totals by that appended denominator.

```{r}
produce %>% 
  group_by(type) %>% 
  mutate(totalGroupCost = sum(cost),
         percOfGroupCost = cost/totalGroupCost) %>% 
  relocate(cost, totalGroupCost, percOfGroupCost)
  
```

### slice_min() and slice_max() 

* A natural question to ask of this dataframe is "What are the most/least expensive item(s)?"
    - A more general version of this question might be "Which rows have the lowest/highest values of a certain variable?"

> `slice_min()` and `slice_max()` return a specified number of the bottom/top rows by some comparable variable.

* The first argument following the dataframe should be a variable to compare by, or multiple if you want to break ties.

* You must also specify the argument `n`, a certain number of rows to return.

```{r}
# What are the three most expensive per-unit items?

# three: n = 3
# most: slice_max, rather than slice_min
# expensive per-unit: Referring to the highest values in the price column

produce %>% 
  slice_max(price, n = 3)
```

* The presence of `group_by()` means the `slice_*` commands will return the top/bottom `n` rows in each group, rather than across the whole dataframe.

* What are the three cheapest items among fruits and among vegetables?

```{r}
# This will return the three cheapest fruits, AND the three cheapest vegetables separately
cheapest = produce %>%
  group_by(type) %>% 
  slice_min(price, n = 3)

cheapest 
```

* Note that slicing does not remove the grouping instruction, because we can still have multiple rows per group. 

### ungroup()

> `ungroup()` removes the grouping instruction put there by a previous `group_by()`.

* For example, let's consider the output of the above chunk from the end of the slice section. It still has a grouping instruction on it.

* If we wanted to calculate the total combined cost of the four items in that list, we could not just take the above code and put a `summarize()` after it, because it would obey the grouping instruction.

```{r}
cheapest %>% 
  summarize(totalCost = sum(cost))
```

* Instead, we need to first take away the grouping instruction with `ungroup()` (no arguments).

```{r}
cheapest %>% 
  ungroup() %>% 
  summarize(totalCost = sum(cost))
```

---

##### EXERCISE: Will it Group?

* Each of the following code chunks contains some code, which has `group_by(type)` commented out.

* For each of the following code chunks:
    + **Run the chunk** with `group_by` commented out.
    + **Predict** what, if anything, will change if you include `group_by`.
    + **Uncomment** `group_by` by removing the hashtag ('#'), run the chunk, and reflect on if you were right!

```{r}
produce %>% 
  #group_by(type) %>% 
  filter(cost > 2)
```

```{r}
produce %>% 
  #group_by(type) %>% 
  mutate(numberOfItems = n()) %>% 
  relocate(numberOfItems)
```

```{r}
produce %>%
  #group_by(type) %>% 
  summarize(medianCost = median(cost)) %>% 
  ungroup()
```

```{r}
produce %>% 
  #group_by(type) %>% 
  slice_max(quantity, n = 2)
```

> Technical takeaway: The presence of `group_by()` can drastically change the output of a dplyr workflow.

> Philosophical takeaway: Separating your commands so there is one on each line makes it easier to debug, both by emphasizing the ORDER the commands are in and allowing you to comment out select ones. 

# Cleaning Up Real Data

- With a basic introduction to dplyr done, we will now illustrate dplyr's application on a real dataset.

- The Lake Mendota data we experimented with last week was in a nice clean format - we did that for you, but that's not how we received it!

- Our aim is to create two new data sets from this raw data set
    - The first keeps one row per interval, but stores the information in more convenient variables
    - The second summarizes the first and retains one summary row per winter (the dataset you know)

## Read in Data

- Begin by reading in the raw, inconvenient Lake Mendota data.
  - There is one row for each interval that Lake Mendota is closed, not each winter (can have multiple per winter)
  - There are fewer variables, and they have different names
  - In particular, the dates are not fully specified
  
- Our objective with this section is to *show* you a real dplyr workflow and show you some new commands, not for you to be at this level yet.

```{r read-data}
mendota_raw = read_csv("../../data/lake-mendota-raw-2022.csv")
head(mendota_raw)

## dimensions
dim(mendota_raw)
```

# Creating the Interval-Level Dataset

## What Does "Interval-Level" Mean?

* We often describe datasets by describing their **level**; this is a fancy term for saying, **"What does a single row of the dataset represent?"**
    - This question is one that people often gloss over, but is actually very important.
    - It requires you to be careful with your wording.

* For our grocery data, one row represented one kind of food item. (Not one purchased item... for example, we have one row that says we have 8 apples, not eight rows that each say "you bought one apple".)

* For this dataset we want to create, one row will represent an *interval of closure*, some of which might take place in the same winter season.

* For the second dataset we want to create, one row will represent one *winter season*, which may have contained multiple closures. 

## dplyr Processing

* Our objectives are to create a new dataframe, `mendota_interval`, with the following changes:
    - Get rid of any rows which have missing data
    - Create `year1` and `year2`
    - Manually compute `duration` of closure ourselves
        - To do this, we'll need to coerce `closed` and `open` into Date objects instead of strings
        
```{r}
# Some winters have multiple intervals, and the first interval does not have number of days calculated.
mendota_raw %>% 
  filter(winter == "1936-37")
```
    
* We're going to do this one step at a time.    
    
* New commands in this section include:
    + `drop_na()`, which gets rid of rows that have missing data in the specified columns. If you give it no arguments, it will assume you want to drop rows with missing data anywhere. (**Important one! Think of it like filter.**)
    + `separate()`, which takes a string and splits it into multiple variables, by default at the first "symbol". (**Not as important.**)
    
```{r}
# Starting from the raw data
mendota_interval = mendota_raw %>%  
  # get rid of rows which have missing data in the closed or open column
  drop_na(closed, open)

# Notice: First three intervals are gone
head(mendota_interval)
```

```{r}
# Starting from the modified dataframe above, get the year1 and year2 numeric variables
mendota_interval = mendota_interval %>% 
  separate(winter,into = c("year1","year2"), remove = FALSE)

# Two new variables have been created
head(mendota_interval)
```

- Note that `year1` and `year2` are both still strings; we can't subtract them yet because R thinks they are "words".
    - Change `year1` to a number and then manually recalculate `year2`
    
- New command in this section:
    - `as.numeric`: Changes a string of numerical digits to a number.
    
```{r}
mendota_interval = mendota_interval %>%
  mutate(year1 = as.numeric(year1),
         year2 = year1 + 1)

head(mendota_interval)
```

- Almost done! Now we just need to calculation `duration`, which will require some manipulation of the dates.

- We need to use the package `lubridate` for this, which we will encounter in week 4.

- New command in this section:
    - `case_when`... important enough that I'm about to take a whole detour to explain it!
    
### case_when()

> `case_when()` is an expression to be used inside `mutate()`, which allows for the complex evaluation of a new column based on another.

* The name for this command comes from how you might describe what it is doing; "In this case, do this. Or, in this case, do this... so on and so forth."

* The general syntax is below, but it might be easier to understand with an example.

```
dataframe %>% 
  mutate(newVariable = case_when(
  
    logical condition 1 ~ value for newVariable when logical condition 1 is TRUE,
    logical condition 2 ~ value for newVariable when logical condition 2 is TRUE,
    ...
    TRUE ~ value for newVariable when none of the above conditions are TRUE
  
  ))
```

* The conditions evaluate in the order you list them, so the first time something matches, it will take that value and not check any further.

```{r}
exampleData = tibble(age = 15:24)
exampleData
```

```{r}
exampleData %>% 
  mutate(legalStatus = case_when(
    age >= 21 ~ "Can vote and drink",
    age >= 18 ~ "Can vote, can't drink",
    TRUE ~ "Can't do either"
    )
  )
```

### Back to Intervals

* We are trying to create the `duration` variable but need to use `case_when()` in our manipulation of the dates, `closed` and `open`.

```{r}
head(mendota_interval)
```

- The variables `closed` and `open` contain the month and day, but the correct year is `year1` for months July through December and `year2` for January through June.

- Any date that does not match either of the first two cases will match `TRUE` and be given the specific character value for missing data, `NA_character_`.
- The function `str_c()` creates a string by combining strings together.
- The function `str_detect()` returns `TRUE` or `FALSE` if a pattern is detected in a string.

```{r}
mendota_interval = mendota_interval %>%
  ## add the correct year to the month and day for closed
  
  mutate(closed = case_when(
    str_detect(closed,"Jul|Aug|Sep|Oct|Nov|Dec") ~ str_c(closed,' ',year1),
    str_detect(closed,"Jan|Feb|Mar|Apr|May|Jun") ~ str_c(closed,' ',year2),
    TRUE ~ NA_character_
  )) %>%
  
  ## then convert the strings to dates with dmy()
  mutate(closed = dmy(closed)) %>%
  
  ## Do the same for open
  mutate(open = case_when(
    str_detect(open,"Jul|Aug|Sep|Oct|Nov|Dec") ~ str_c(open,' ',year1),
    str_detect(open,"Jan|Feb|Mar|Apr|May|Jun") ~ str_c(open,' ',year2),
    TRUE ~ NA_character_
  )) %>%
  mutate(open = dmy(open)) %>% 
  
  ## recalculate the number of days closed with ice
  mutate(duration = as.numeric(open - closed))

head(mendota_interval)
```

* We are done! From here, we can create the season-level dataset.

# Creating the Season-Level Dataset

## Checking Intervals

- From the original call to `dim(mendota_raw)`, there are 177 rows in the interval-level dataset.

- Check with the console that `2022-1852` (the number of unique start years) is less than 177.

- This must mean we have *more intervals than winters*, which means *some winters must have more than one interval!*

- **Which winters have multiple intervals?**
    - Plan of attack: It would be nice if we had a dataframe that was at the level of each winter, and a variable that said how many intervals that winter had. If we had that, we could just `filter` it.
    - To get from our current interval-level dataset to a dataset where one row represents one winter, we need to `group_by(winter)`.
    - To calculate how many times a given winter appears in the interval-level dataset, we can `summarize(numIntervals = n())`.

```{r}
# If you're in RMarkdown... go to "page 9" of this dataframe!
mendota_interval %>% 
  group_by(winter) %>% 
  summarize(numIntervals = n())
```

- Now let's use `filter()` to determine which winters have 2 or more closures!

```{r}
# Recall count() is the same as group_by() %>% summarize(n = n())
mendota_interval %>% 
  group_by(winter) %>% 
  summarize(numIntervals = n()) %>% 
  filter(numIntervals >= 2)
```

- There are seven winters with two freeze intervals.
    
## Summarizing From Intervals to Winters

- This syntax is actually the framework to create the other important variables on the *winter* level rather than the *interval* level as well!

- Recall that most winters just have one interval, so these calculations are somewhat trivial for those, but they are meaningful for the winters with two intervals.
    - `duration` is the sum of all of the durations for the intervals in that winter
    - `first_freeze` is the earliest close date
    - `last_thaw` is the latest open date

* Recall that `summarize()` gets rid of everything EXCEPT the grouping variable and those created in summarize.

* We want to keep `year1` and `year2`, in addition to winter... so we can cautiously exploit this behavior. Because every winter has a single unique `year1` and a single unique `year2`, we can include them in the `group_by` so they are kept, without changing the final structure.
    + *This is a somewhat complex behavior; we're going to take a deeper look at this later in these lecture notes, don't stress too much about it right now.*

```{r}
mendota = mendota_interval %>% 
  group_by(winter, year1, year2) %>% 
  summarize(intervals = n(),
            duration = sum(duration),
            first_freeze = min(closed),
            last_thaw = max(open))

head(mendota)
```

- Add the `decades` variable with mutate.
    - `floor()` rounds down to the nearest integer
    - By dividing by 10, rounding down, and multiplying again by ten, we round down to the nearest decade.

```{r}
mendota = mendota %>%
  mutate(decade = floor(year1 / 10) * 10)

head(mendota)
```

- Finally, use `relocate()` to change the column order

```{r}
mendota = mendota %>%
  relocate(winter, year1, year2, decade)

head(mendota)
```

## Combining the steps

- In practice, we may put all of the code to transform these data into a single R chunk, in one or two long chains connected by pipes.
    - We would build the code by doing each step and verifying the results before adding the next one.
    
```{r, eval = FALSE}
## transform interval data
mendota_interval = mendota_raw %>%
  select(-days) %>% 
  drop_na() %>% 
  separate(winter,into = c("year1","year2"), remove = FALSE) %>% 
    mutate(year1 = as.numeric(year1)) %>%
  mutate(year2 = year1+1) %>% 
      mutate(closed = case_when(
    str_detect(closed,"Jul|Aug|Sep|Oct|Nov|Dec") ~ str_c(closed,' ',year1),
    str_detect(closed,"Jan|Feb|Mar|Apr|May|Jun") ~ str_c(closed,' ',year2),
    TRUE ~ NA_character_
  )) %>%
  mutate(closed = dmy(closed)) %>%
  mutate(open = case_when(
    str_detect(open,"Jul|Aug|Sep|Oct|Nov|Dec") ~ str_c(open,' ',year1),
    str_detect(open,"Jan|Feb|Mar|Apr|May|Jun") ~ str_c(open,' ',year2),
    TRUE ~ NA_character_
  )) %>%
  mutate(open = dmy(open)) %>% 
  mutate(duration = as.numeric(open - closed))

## summarize per year
mendota = mendota_interval %>% 
  group_by(winter, year1, year2) %>% 
  summarize(intervals = n(),
            duration = sum(duration),
            first_freeze = min(closed),
            last_thaw = max(open)) %>% 
     mutate(decade = floor(year1 / 10) * 10) %>% 
    select(winter, year1, year2, decade, everything())
```
    
* We've completed a real data cleaning process with dplyr, and learned a couple new commands along the way!

* We now introduce another important dataset for the class, and practice dplyr concepts further.
    
# Madison Weather Overview

- We will be working with data set containing weather data on Madison collect over a number of years from  different weather stations.
- In addition to investigating different scientific questions on the data, we will also be learning about the [`dplyr` package](https://dplyr.tidyverse.org/) from `tidyverse` which is "a grammar of data manipulation."  

## Overview of Madison Weather Data

> The source of the data for this week is from the National Oceanic and Atmospheric Administration (NOAA) of the United States.

- See [CNCS Chapter 7](https://bookdown.org/bret_larget/stat-240-case-studies/madison-weather.html) for more information.
- The data set described here includes data through December 31, 2022.

- The data file is `madison-weather-1869-2022.csv`, and the variable descriptions are provided below.

STATION: unique code for the weather station   
NAME: station name    
LATITUDE: station latitude  
LONGITUDE: station longitude  
ELEVATION: station elevation (feet above sea level)   
DATE: date of observations  
PRCP: precipitation (inches, water)  
SNOW: snow fall (inches)  
SNWD: snow depth (inches)  
TAVG: daily average air temperature (degrees Fahrenheit)   
TMAX: maximum air temperature (degrees Fahrenheit)   
TMIN: minimum air temperature (degrees Fahrenheit)   

## Read in Data

- The data file has one row for each date and weather station.
- The following block of code will read in the data.
    - We specify the types of data in each column.
    - This prevents an error where some variables with many initial rows of contain missing data and are set to be logical instead of numeric.
    
* This data is at the level of **station-day**. There are four unique stations in the data, each of which records information every day. Some rows will have the same station, some rows will have the same day, but NONE will share both.
    
```{r}
## Read in the data
## Specify the type
## This is needed to avoid issues for columns where first 1000 rows are all missing
##   and the type is set to logical

mw_raw = read_csv("../../data/madison-weather-1869-2022.csv",
                       col_types = cols(
                         STATION = col_character(),
                         NAME = col_character(),
                         LATITUDE = col_double(),
                         LONGITUDE = col_double(),
                         ELEVATION = col_double(),
                         DATE = col_date(format = ""),
                         PRCP = col_double(),
                         SNOW = col_double(),
                         SNWD = col_double(),
                         TAVG = col_double(),
                         TMAX = col_double(),
                         TMIN = col_number()))
dim(mw_raw)

head(mw_raw)
```

## Transform Data

- It is generally best practice to avoid using all caps for variable names; these are reserved for important constant values.
    - Let's change names using the `rename(new_name = old_name)` function. 

```{r}
mw = mw_raw %>%
  rename(station = STATION,
         name = NAME,
         latitude = LATITUDE,
         longitude = LONGITUDE,
         elevation = ELEVATION,
         date = DATE,
         prcp = PRCP,
         snow = SNOW,
         snow_depth = SNWD,
         tavg = TAVG,
         tmax = TMAX,
         tmin = TMIN)
```

- The values in the `name` variable are long and unwieldy.
- Change them using `case_when()` inside of `mutate()`.

```{r}
# Remember each "case" is in the form: logical condition ~ new 
mw = mw %>% 
  mutate(name = case_when(
    name == "UW ARBORETUM MADISON, WI US" ~ "Arboretum",
    name == "CHARMANY FARM, WI US" ~ "Charmany",
    name == "MADISON DANE CO REGIONAL AIRPORT, WI US" ~ "Airport",
    name == "MADISON WEATHER BUREAU CITY, WI US" ~ "Bureau"
  ))
```

# Summarizing: Weather Stations

- Let's make a summary with one row per weather station where we can specify:
    - The station code
    - The earliest date included
    - The latest date included
    - The number of dates included in the data set
    - The number of dates that are missing
- Here is the strategy:
    - use `group_by()` to do the calculations for each weather station separately
    - Compare the total number of possible dates from the earliest to latest dates with the actual number of values for each
    
```{r}
mw %>% 
  group_by(name) %>% 
  summarize(first_date = min(date),
            last_date = max(date),
            n = n(),
            # Note these last two variables are not "reduction functions" directly. However, they exclusively refer to the OUTPUT of reduction functions, so they are indirectly also reduction functions.
            possible = as.numeric(last_date - first_date) + 1,
            missing = possible - n)
```

## Advanced group_by(): Keeping Constant Variables, or Multiple Groupings

* We briefly mentioned this with the Lake Mendota data, but notice in the output above that a lot of useful information about each station; such as its `station` code, `latitude`, `longitude`, and `elevation` are now gone, because we did not `group_by()` them nor compute them in `summarize()`.

* However, because those variables are **constant** with regards to each station's `name`, we can include them in `group_by` without changing the structure of the final summary.

> Grouping by multiple variables will generate summary information for all rows that share a value for EVERY grouping variable. This behavior can be exploited when a set of constant values always shows up together, to keep all of them in the dataset after summarizing.  

* The rows that share `name` also, by definition HAVE to share the other variables.

```{r}
stations = mw %>% 
  group_by(name, station, latitude, longitude, elevation) %>% 
  summarize(first_date = min(date),
            last_date = max(date),
            n = n(),
            # Note these last two variables are not "reduction functions" directly. However, they exclusively refer to the OUTPUT of reduction functions, so they are indirectly also reduction functions.
            possible = as.numeric(last_date - first_date) + 1,
            missing = possible - n)

stations
```

* However, consider a variable like `snow`. There may be rows that share a value of `name`, but do NOT share a value of `snow`. We cannot just include this in `group_by`.

* We could include an AVERAGE or TOTAL amount of snow for that station... but you already know how to do that with `summarize()`, not `group_by()`!

* Here's what happens when you do include `snow` in the `group_by`... there are 27,355 rows that share `name == "Airport"` and `snow == 0.0`, there are 473 rows that share `name == "Airport"` and `snow == 0.1`... on page 29, there are 37 rows that share `name == "Charmany"` and `snow == 2.5`... so on and so forth, and they are kept separate.

```{r}
mw %>% 
  group_by(name, snow) %>% 
  summarize(first_date = min(date),
            last_date = max(date),
            n = n(),
            # Note these last two variables are not "reduction functions" directly. However, they exclusively refer to the OUTPUT of reduction functions, so they are indirectly also reduction functions.
            possible = as.numeric(last_date - first_date) + 1,
            missing = possible - n)
```

* Sometimes, grouping by multiple variables is exactly what we want to do; but not here. We'll see such an example in Lecture Questions.

## Back to Stations
    
```{r}
stations
```
    
- This summary data matches up with a known structure: the official weather station was the Madison Weather Bureau from January 1, 1869 until September 30, 1939 and then the Airport from October 1, 1939 to the present.
- The weather stations at the Arboretum and Charmany Farms were never official, and have a lot of missing data.
- We will modify the `mw` data set by:
    - keeping only `name` and daily weather variables
    - keeping only the *station-dates* which represent the "official" data.
    - arranging by date
- We can create an official Madison weather data set with one observation per date

```{r}
official = mw %>%
  select(name, date, prcp, snow, snow_depth, tmin, tmax, tavg) %>% 
  filter(name == "Airport" | (name == "Bureau" & date < ymd("1939-10-01"))) %>%
  arrange(date, name)
```

- Let's redo the missing data calculations while each station was "official".

```{r}
official_stations = official %>% 
  select(name, date) %>% 
  group_by(name) %>% 
  summarize(first_date = min(date),
            last_date = max(date),
            n = n(),
            possible = as.numeric(last_date - first_date) + 1,
            missing = possible - n)
official_stations
```

- The official record is missing just seven days. Let's find them!

# Missing Data

* When we say the record is *missing* seven days, that is different from when we talk about a row having missing data.

* For example, missing data in a ROW looks like this, where snow-related variables were simply not recorded early in the data collection process.

```{r}
head(official)
```

* When we say the record has dates missing, it is not that there is a placeholder and an `NA`; that row simply never appears in the data.

* For example, May 21st, 1905 simply does not appear in the data.

```{r}
official %>% 
  filter(date > ymd("1905-05-18") & date < ymd("1905-05-24"))
```

- To add these missing dates in, we will create a data frame with all dates from January 1, 1869 through September 30, 1939, and then `full_join` (keeps all rows from both dataframes) it to the official weather data set.

```{r}
## Create a temporary data file
temp = tibble(
  date = seq(ymd("1869-01-01"), ymd("1939-09-30"), 1),
  name = "Bureau")

## join with the official data set
## show this adds the seven missing dates
nrow(official)

official = official %>% 
  full_join(temp, by = c("name", "date"))

## Note there are now seven more dates added
nrow(official)

## Eliminate the temporary data set
rm(temp)
```

- Next, let's look at the column-wise levels of missing data.

## Sidenote: Counting With Logical Conditions

- We would love to have a summary function which counts the number of NA's.

- This does not directly exist, but we can combine two functions to recreate it.

- First, R understands that if you pass a logical vector into a numeric reduction function, you want it to treat `TRUE` as "1" and `FALSE` as 0.

- This allows you to run a logical vector through `sum()` to count the number of `TRUE`s, or `mean()` for the percentage that are `TRUE`.

```{r}
logicals = c(TRUE, FALSE, TRUE, TRUE, FALSE)

sum(logicals)
mean(logicals)
```

* Combining this principle with `is.na()` to create the logical vector, we can compute how many or what percent of values are missing in a vector.

```{r}
someMissing = c(1, NA, 2, NA, 3)

sum(is.na(someMissing))
mean(is.na(someMissing))
```

## Back to Weather

- Using the above principle to count NA values:

```{r}
# Note: use of summarize() without group_by() calculates these reduction functions across the whole dataframe, which is what we want
official_summary = official %>% 
  summarize(prcp_na = sum(is.na(prcp)),
            snow_na = sum(is.na(snow)),
            depth_na = sum(is.na(snow_depth)),
            tmax_na = sum(is.na(tmax)),
            tmin_na = sum(is.na(tmin)),
            tavg_na = sum(is.na(tavg)))

official_summary
```

- There is a lot of missing data for average temperatures.
- We can recalculate these as the average of the `tmax` and `tmin` variables.
- There is minimal missing data for precipitation or maximum or minimum temperatures.
    - Summaries of these variables over the years after dropping missing data are not likely to be too inaccurate, especially if we just make the educated guess that `tavg` was around the average of `tmin` and `tmax`, which are almost always present in the data.
    
```{r}
official = official %>% 
  mutate(tavg = (tmin + tmax) / 2)
```    
    
- Snow and snow depth data has a fair amount missing.
    - Much of this is because the data was not recorded at the beginning of the collection period.
  
#### Snow data

- Let's take a closer look at the pattern of missing data for the two snow variables

```{r}
official %>% 
  select(name, date, snow) %>% 
  drop_na() %>% 
  summarize(first_snow = min(date))

official %>% 
  select(name, date, snow_depth) %>% 
  drop_na() %>% 
  summarize(first_snow_depth = min(date))
```
    
- If we analyze the snow data, we will need to use a different time range.
- Also, some NA snow totals in the summer or when precipitation is zero may really be zero and not missing.
- We can count missing data by month and then plot the results

```{r}
snow_sum = official %>% 
  mutate(month = month(date, label = TRUE)) %>% ## here, month() is a lubridate command
  group_by(month) %>% 
  summarize(n = n(),
            missing = sum(is.na(snow)),
            p = missing / n)

head(snow_sum)

ggplot(snow_sum, aes(x = month, y = p)) +
  geom_col(fill = "blue") +
  scale_y_continuous(labels = scales::percent) +
  xlab("Month") +
  ylab("Percent of Missing Data") +
  ggtitle("Madison weather: daily snowfall missing data percentages by month",
          subtitle = "1869 - 2021")
```


```{r}
snow_sum2 = official %>% 
  mutate(year = year(date)) %>%
  group_by(year) %>% 
  summarize(n = n(),
            missing = sum(is.na(snow)),
            p = missing / n)

head(snow_sum2)

ggplot(snow_sum2, aes(x = year, y = p)) +
  geom_segment(aes(x = year, xend = year, yend = 0 )) +
  scale_y_continuous(labels = scales::percent) +
  xlab("Year") +
  ylab("Percent of Missing Data") +
  ggtitle("Madison weather: daily snowfall missing data percentages by year",
          subtitle = "1869 - 2021")

## Years since 1900
snow_sum2 %>% 
  filter(year > 1899) %>% 
ggplot(aes(x = year, y = p)) +
  geom_segment(aes(xend = year, yend = 0 )) +
  scale_y_continuous(labels = scales::percent) +
  xlab("Year") +
  ylab("Percent of Missing Data") +
  ggtitle("Madison weather: daily snowfall missing data percentages by year",
          subtitle = "1869 - 2021")
```

## Save Data

- It will be helpful to write the modified official weather data into a file so that we do not need to redo the data transformations each time.

```{r, eval = TRUE}
write_csv(official,
          "../../data/madison-weather-official-1869-2022.csv")
```

# Lecture Questions

- In this lecture, we will pose a number of questions and then work interactively to solve as many as we can.
- The notes will include solutions for the questions we do not get to.

## Read in Processed Data

- Begin by reading in the official weather data that we just created above.

```{r}
## read in the data
official = read_csv("../../data/madison-weather-official-1869-2022.csv")
```

## Annual Average Temperature

> Create a summary table of the data with the average temperature per year. Then, make an effective plot of this data to show patterns over time and variation around the main pattern.

- We don't have a `year` variable; will need to create that.

- There were 14 dates with missing `tavg`... what to do about those?

- Our current dataset is at the `date` level; we need to summarize that to the `year` level.

- Then use last week's concepts to graph the data

```{r}
# Create the year variable
# Remove dates with missing tavg
# Summarize to the level of year
```

```{r}
# Year should be on the x axis, average temperature in that year should be on the y axis. Include points, a connecting line, and a curved trend line. Include proper labels.
```

- The pattern we see is:
    - the average temperature increased from about 45.5 degrees Fahrenheit in 1870 up to about 46.8 degrees around 1940.
    - then, temperatures went down for about 30 years, bottoming out in 1970 at about 45.9 degrees
    - since then, the average temperature has increased over 2 degrees Fahrenheit
    - the rate that the average temperature is increasing is accelerating

## Daily Temperature Records

> For each of the 366 unique days of the year, find the historical date which had the highest maximum temperature on that day of the year. There may be ties, which will lead to slightly more than 366 rows.

- Once again, starting from `official` we don't have `year`, `month`, or `day` and will need to create them.

- We will have to evaluate which row in `official` has the highest `tmax` within each combination of `month` and `day`.

- We can do that with `group_by`, leading into `slice_max`. 

```{r}

```

## Temperature by 30-year period

> Meteorologists often determine weather norms by averaging over a 30-year period. Create a variable which partitions the data into 30-year periods from 1871-1900, 1901-1930, up to 1991-2020. You can exclude years before 1871 and after 2020.

> Then calculate within each of the twelve months within each period (e.g. 1 row for all Januarys in 1871-1900, 1 row for all Februarys in 1871-1900...) the average, maximum, and minimum of the average temperature.


- Take a moment to make sure you understand what the question is asking for. We have 5 periods and there are twelve unique months in each period (e.g. January 1971 is lumped together with January 1972) so we will get 60 rows.

- Once again, starting from `official` we don't have `year` and will need to create it, as well as `month`.

- We'll also need to only keep those rows between 1871 and 2020.

- Creating a new `period30` variable is a complex operation based on the year; let's employ `case_when`.

```{r}
# Create year and month
# Filter to years between 1871 and 2020
# Create period30 with case_when
```

```{r}
# Summarize to the month-period level
```

## Days with Precipitation

> Make a summary table which shows the proportion of days which had any precipitation in each 30-year period + month combination.

```{r}

```

> With a separate panel for each month, create a column graph of the percentage of precipitation in each 30-year period.

```{r}

```



